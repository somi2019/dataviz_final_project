---
title: "Visualizing Text and Distributions"
output: 
  html_document:
    keep_md: true
    toc: true
    toc_float: true
---

# Data Visualization Project 03


In this exercise you will explore methods to visualize text data and practice how to recreate charts that show the distributions of a continuous variable. 


## Part 1: Density Plots
Using the dataset obtained from FSU's [Florida Climate Center](https://climatecenter.fsu.edu/climate-data-access-tools/downloadable-data), for a station at Tampa International Airport (TPA) from 2016 to 2017, attempt to recreate the charts shown below

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
weather_tpa <- read_csv("https://github.com/reisanar/datasets/raw/master/tpa_weather_16_17.csv")
# random sample 
sample_n(weather_tpa, 4)
```

See https://www.reisanar.com/slides/relationships-models#10 for a reminder on how to use this dataset with the `lubridate` package for dates and times.


(a) Recreate the plot below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_facet.png")
```

Hint: the option `binwidth = 3` was used with the `geom_histogram()` function.

(b) Recreate the plot below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_density.png")
```

Hint: check the `kernel` parameter of the `geom_density()` function, and use `bw = 0.5`.

(c) Recreate the chart below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_density_facet.png")
```

Hint: default options for `geom_density()` were used. 

(d) Recreate the chart below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_ridges.png")
```

Hint: default options for `geom_density()` were used. 

(e) Recreate the plot below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_ridges.png")
```

Hint: use the`ggridges` package, and the `geom_density_ridges()` function paying close attention to the `quantile_lines` and `quantiles` parameters.

(f) Recreate the chart below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_ridges_plasma.png")
```

Hint: this uses the `plasma` option (color scale) for the _viridis_ palette.




## Part 2: Visualizing Text Data


```{r}
library(tidyverse)
billboard<-read_csv("https://raw.githubusercontent.com/reisanar/datasets/master/BB_top100_2015.csv")
```


```{r}
library(tidytext)
```


```{r}
head(billboard,8)
```



```{r}
#To see the columns in the data frame:

names(billboard) 
 
```

```{r}
dim(billboard)
```
```{r}
#To see how they are structured,choose lyrics column for one of the songs as a sample
str(billboard[100, ]$Lyrics, nchar.max = 50)
```
```{r}
# convert everything to lower case
billboard$Lyrics <- sapply(billboard$Lyrics, tolower)
```


```{r}
summary(billboard)
```

```{r}
billboard <- billboard %>%
  mutate(chart_level = ifelse(billboard$Rank %in% 1:10, "Top 10", 
           ifelse(billboard$Rank %in% 11:100, "Top 100", "Uncharted")))
```


```{r}
#Filtering top 10 songs:
billboard %>%
  filter(Rank <= 10 )
  
```


After creating plots I noticed there were some redundant characters in the text,so I tried to git rid of them by bellow function:
```{r}
Redundant_words <- c("meaning","youre","wanna","byjamesg","gotta","gonna","nigga","didnt","whats","niggas","bitch","dont","yeah","aint","wont")
```

Also using **stop_words** function to filter some specific words:
```{r}
head(sample(stop_words$word, 30), 30)

```

```{r}
##unnest and remove stop, undesirable and short words

billboard <- billboard %>%
  unnest_tokens(word, Lyrics) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(!word %in% Redundant_words) %>%
  filter(nchar(word) >= 4)
```


#Look for top words frequency


#Top Words
In order to do a simple evaluation of the most frequently used words in the full set of lyrics, you can use count() and top_n() to get the n top words from your clean, filtered dataset. 

```{r}
billboard %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
    geom_col(aes(word, n), fill ="pink") +
    theme(legend.position = "none", 
          plot.title = element_text(hjust = 0.5),
          panel.grid.major = element_blank()) +
    ylab("Song Count") +
    ggtitle("Most Frequently Used Words in billboard Lyrics") +
    coord_flip()
```


#Popular Words


```{r}
popular_words <- billboard %>% 
  group_by(chart_level) %>%
  count(word, chart_level, sort = TRUE) %>%
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(chart_level,n) %>%
  mutate(row = row_number()) 

popular_words %>%
  ggplot(aes(row, n, fill = chart_level)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "Song Count") +
    ggtitle("Popular Words by Chart Level") + 
    facet_wrap(~chart_level, scales = "free") +
    scale_x_continuous(  # This handles replacement of row 
      breaks = popular_words$row, # notice need to reuse data frame
      labels = popular_words$word) +
    coord_flip()

```

```{r}
popular_tfidf_words <- billboard %>%
  unnest_tokens(word, Song) %>%
  distinct() %>%
  filter(!word %in% Redundant_words) %>%
  filter(nchar(word) >= 4) %>%
  count(chart_level, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, chart_level, n)
head(popular_tfidf_words)
```
#TF-IDF
 Using TF-IDF certainly gives us a different perspective on potentially important words.

```{r}
top_popular_tfidf_words <- popular_tfidf_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(chart_level) %>% 
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(chart_level, tf_idf) %>%
  mutate(row = row_number())

top_popular_tfidf_words %>%
  ggplot(aes(x = row, tf_idf, 
             fill = chart_level)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "TF-IDF") + 
    ggtitle("Important Words using TF-IDF by Chart Level") +
    facet_wrap(~chart_level, ncol = 3, scales = "free") +
    scale_x_continuous(  # This handles replacement of row 
      breaks = top_popular_tfidf_words$row, # notice need to reuse data frame
      labels = top_popular_tfidf_words$word) +
    coord_flip()
```

Conclusion
In this project, after checking the basics features and performing some actions such as data cleansing and removing uninformative words,an exploratory analysis was begun to categorize song levels.

Next,text visualizing was checked by unnesting lyrics into tokenized words so it was possible to look at lyric complexity. The results provide critical insights for the next steps of **sentiment analysis** and topic modeling.

Finally,**TF-IDF analysis**  was used to represent the information behind a word in a document relating to some outcome of interest. Result may look interesting, but it could be only some part od the whole story.
* use of **slice(seq_len(n))** to grab the first n words in each chart_level. 
* use **bind_tf_idf()** to run the formulas and create new columns.



